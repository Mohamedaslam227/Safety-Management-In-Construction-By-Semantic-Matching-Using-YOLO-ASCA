{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATING SINGLE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_labels(label_file, class_mapping):\n",
    "    \"\"\"\n",
    "    Update the class labels in the label file by using a class mapping.\n",
    "    \n",
    "    Args:\n",
    "        label_file (str): Path to the label file.\n",
    "        class_mapping (dict): Dictionary mapping original class labels to new ones.\n",
    "    \n",
    "    Returns:\n",
    "        str: Updated content of the label file.\n",
    "    \"\"\"\n",
    "    updated_content = []\n",
    "    with open(label_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            class_label = class_mapping[int(parts[0])]\n",
    "            updated_content.append(f\"{class_label} \" + \" \".join(parts[1:]))\n",
    "    \n",
    "    return \"\\n\".join(updated_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_yaml_file(output_dir, num_classes, class_names):\n",
    "    \"\"\"\n",
    "    Create a YAML configuration file for the combined dataset.\n",
    "    \n",
    "    Args:\n",
    "        output_dir (str): Output directory path for the combined dataset.\n",
    "        num_classes (int): Total number of classes in the combined dataset.\n",
    "        class_names (list): List of class names.\n",
    "    \"\"\"\n",
    "    yaml_content = f\"\"\"\n",
    "# Combined Dataset Configuration\n",
    "path: {output_dir}\n",
    "train: {output_dir}/train/images\n",
    "val: {output_dir}/val/images\n",
    "test: {output_dir}/test/images\n",
    "\n",
    "# Number of classes\n",
    "nc: {num_classes}\n",
    "\n",
    "# Class names\n",
    "names: [{', '.join([f\"'{name}'\" for name in class_names])}]\n",
    "\"\"\"\n",
    "    yaml_path = os.path.join(output_dir, \"dataset_config.yaml\")\n",
    "    with open(yaml_path, 'w') as yaml_file:\n",
    "        yaml_file.write(yaml_content)\n",
    "    print(f\"YAML configuration file created at: {yaml_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_datasets(base_dir, dataset_dirs, num_classes_list, class_names, output_dir):\n",
    "    current_max_class = 0\n",
    "    global_image_counter = 1  \n",
    "    for folder in ['train', 'val', 'test']:\n",
    "        os.makedirs(os.path.join(output_dir, folder, 'images'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_dir, folder, 'labels'), exist_ok=True)\n",
    "    \n",
    "    for idx, dataset in enumerate(dataset_dirs):\n",
    "        print(f\"Processing dataset: {dataset}\")\n",
    "        num_classes = num_classes_list[idx]\n",
    "        class_mapping = {i: i + current_max_class for i in range(num_classes)}\n",
    "\n",
    "        for folder in ['train', 'val', 'test']:\n",
    "            image_dir = os.path.join(base_dir, dataset, folder, 'images')\n",
    "            label_dir = os.path.join(base_dir, dataset, folder, 'labels')\n",
    "            if not os.path.exists(image_dir) or not os.path.exists(label_dir):\n",
    "                print(f\"Skipping {folder} folder for {dataset} as it does not exist.\")\n",
    "                continue\n",
    "\n",
    "            for image_path in glob(os.path.join(image_dir, '*')):\n",
    "                unique_image_name = f\"{global_image_counter}.jpg\"\n",
    "                dest_image_path = os.path.join(output_dir, folder, 'images', unique_image_name)\n",
    "                shutil.copy(image_path, dest_image_path)\n",
    "                label_path = os.path.join(label_dir, os.path.splitext(os.path.basename(image_path))[0] + '.txt')\n",
    "                if os.path.exists(label_path):\n",
    "                    unique_label_name = f\"{global_image_counter}.txt\" \n",
    "                    dest_label_path = os.path.join(output_dir, folder, 'labels', unique_label_name)\n",
    "                    updated_content = update_labels(label_path, class_mapping)\n",
    "                    \n",
    "                    with open(dest_label_path, 'w') as f:\n",
    "                        f.write(updated_content)\n",
    "\n",
    "                global_image_counter += 1 \n",
    "        current_max_class += num_classes\n",
    "    generate_yaml_file(output_dir, current_max_class, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r\"E:\\My Research Project\\CODE\\DATA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dirs = [r\"Excavators\", r\"People and Ladders\",r\"Personal Protective Equipment\",r\"COCO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = r\"E:\\My Research Project\\CODE\\DataCombined\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes_list = [3,2,14,80]\n",
    "class_names = [['EXCAVATORS', 'dump truck', 'wheel loader'],['Ladder', 'Person'],['Fall-Detected', 'Gloves', 'Goggles', 'Hardhat', 'Ladder', 'Mask', 'NO-Gloves', 'NO-Goggles', 'NO-Hardhat', 'NO-Mask', 'NO-Safety Vest', 'Person', 'Safety Cone', 'Safety Vest'],['aeroplane', 'apple', 'backpack', 'banana', 'baseball bat', 'baseball glove', 'bear', 'bed', 'bench', 'bicycle', 'bird', 'boat', 'book', 'bottle', 'bowl', 'broccoli', 'bus', 'cake', 'car', 'carrot', 'cat', 'cell phone', 'chair', 'clock', 'cow', 'cup', 'diningtable', 'dog', 'donut', 'elephant', 'fire hydrant', 'fork', 'frisbee', 'giraffe', 'hair drier', 'handbag', 'horse', 'hot dog', 'keyboard', 'kite', 'knife', 'laptop', 'microwave', 'motorbike', 'mouse', 'orange', 'oven', 'parking meter', 'person', 'pizza', 'pottedplant', 'refrigerator', 'remote', 'sandwich', 'scissors', 'sheep', 'sink', 'skateboard', 'skis', 'snowboard', 'sofa', 'spoon', 'sports ball', 'stop sign', 'suitcase', 'surfboard', 'teddy bear', 'tennis racket', 'tie', 'toaster', 'toilet', 'toothbrush', 'traffic light', 'train', 'truck', 'tvmonitor', 'umbrella', 'vase', 'wine glass', 'zebra']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: Excavators\n",
      "Processing dataset: People and Ladders\n",
      "Processing dataset: Personal Protective Equipment\n",
      "Processing dataset: COCO\n",
      "Skipping val folder for COCO as it does not exist.\n",
      "YAML configuration file created at: E:\\My Research Project\\CODE\\DataCombined\\dataset_config.yaml\n",
      "Datasets combined successfully.\n"
     ]
    }
   ],
   "source": [
    "combine_datasets(path, dataset_dirs, num_classes_list, class_names, output_dir)\n",
    "print(\"Datasets combined successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AslamGPUPyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
