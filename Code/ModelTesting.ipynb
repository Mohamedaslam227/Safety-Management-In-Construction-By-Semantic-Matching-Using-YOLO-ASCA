{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.resnet import resnet101\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "from gensim.models import KeyedVectors\n",
    "from torchvision import transforms\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Running on cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Paths\n",
    "DATASET_PATH = r\"E:\\My Research Project\\CODE\\DATA\\Combined_Dataset\"\n",
    "SAFETY_RULES_PATH = r\"E:\\My Research Project\\CODE\\Saftey Rules-OG.txt\"  # Safety rules file\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = 19  # Adjust according to your dataset\n",
    "print(f\"Model Running on {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images_path, labels_path, transform=None):\n",
    "        self.images_path = images_path\n",
    "        self.labels_path = labels_path\n",
    "        self.image_files = [f for f in os.listdir(images_path) if f.endswith(('.jpg', '.png'))]\n",
    "        self.label_files = [f for f in os.listdir(labels_path) if f.endswith('.txt')]\n",
    "        self.transform = transform if transform else transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.images_path, self.image_files[idx])\n",
    "        label_path = os.path.join(self.labels_path, self.label_files[idx])\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Failed to load image: {img_path}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = self.transform(image)\n",
    "        with open(label_path, \"r\") as f:\n",
    "            labels = [list(map(float, line.strip().split())) for line in f]\n",
    "\n",
    "        boxes = []\n",
    "        classes = []\n",
    "        for label in labels:\n",
    "            class_id, x_center, y_center, width, height = label\n",
    "            if width > 0 and height > 0:  # Check for valid bounding boxes\n",
    "                xmin = x_center - width / 2\n",
    "                ymin = y_center - height / 2\n",
    "                xmax = x_center + width / 2\n",
    "                ymax = y_center + height / 2\n",
    "                boxes.append([xmin, ymin, xmax, ymax])\n",
    "                classes.append(int(class_id))\n",
    "        if len(boxes) == 0:  # Handle cases with no valid bounding boxes\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        classes = torch.tensor(classes, dtype=torch.int64)\n",
    "        target = {\"boxes\": boxes, \"labels\": classes}\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(dataset_path, mode, batch_size=2):\n",
    "    images_path = os.path.join(dataset_path, mode, \"images\")\n",
    "    labels_path = os.path.join(dataset_path, mode, \"labels\")\n",
    "    dataset = CustomDataset(images_path, labels_path, transform=transforms.ToTensor())\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes):\n",
    "    backbone = resnet101(pretrained=False)\n",
    "    backbone = nn.Sequential(*list(backbone.children())[:-2])  # Remove fully connected layers\n",
    "    backbone.out_channels = 2048  # Output channels of ResNet-101\n",
    "\n",
    "    anchor_generator = AnchorGenerator(\n",
    "        sizes=((32, 64, 128, 256, 512),),\n",
    "        aspect_ratios=((0.5, 1.0, 2.0),) * 5\n",
    "    )\n",
    "\n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
    "        featmap_names=['0'], output_size=7, sampling_ratio=2\n",
    "    )\n",
    "\n",
    "    model = FasterRCNN(\n",
    "        backbone,\n",
    "        num_classes=num_classes,\n",
    "        rpn_anchor_generator=anchor_generator,\n",
    "        box_roi_pool=roi_pooler\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Embedding with GloVe\n",
    "class TextEmbedding(nn.Module):\n",
    "    def __init__(self, glove_file=r\"E:\\My Research Project\\CODE\\glove.6B.300d.txt\", embedding_dim=300):\n",
    "        super(TextEmbedding, self).__init__()\n",
    "        print(f\"Loading GloVe embeddings from {glove_file}...\")\n",
    "        self.glove_model = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.fc = nn.Linear(embedding_dim, 128)\n",
    "\n",
    "    def forward(self, text_input):\n",
    "        embeddings = [self.glove_model[word] for word in text_input.split() if word in self.glove_model]\n",
    "        embeddings = torch.tensor(embeddings, dtype=torch.float32)\n",
    "        embeddings = embeddings.mean(dim=0)  # Average embeddings for input text\n",
    "        return self.fc(embeddings.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.frcnn = get_model(num_classes)\n",
    "        self.text_embedding = TextEmbedding()\n",
    "        self.concat_fc = nn.Linear(2048 + 128, num_classes)\n",
    "\n",
    "    def forward(self, images, targets=None, safety_text=None):\n",
    "        if self.training:\n",
    "            losses = self.frcnn(images, targets)\n",
    "            return losses\n",
    "        else:\n",
    "            detections = self.frcnn(images)\n",
    "            if safety_text is not None:\n",
    "                text_features = self.text_embedding(safety_text)\n",
    "                combined_features = torch.cat((detections[\"features\"], text_features), dim=1)\n",
    "                return self.concat_fc(combined_features)\n",
    "            return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load Model from Checkpoint\n",
    "def load_model_from_checkpoint(checkpoint_path, num_classes):\n",
    "    model = CombinedModel(num_classes=num_classes).to(DEVICE)\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint[\"model_state\"])\n",
    "    print(\"Model loaded successfully from checkpoint.\")\n",
    "    model.eval()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, data_loader, max_images=50):\n",
    "    \"\"\"\n",
    "    Test the model and collect predictions and targets.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    all_targets = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (images, targets) in enumerate(data_loader):\n",
    "            if idx >= max_images:  # Limit to a maximum number of images\n",
    "                break\n",
    "\n",
    "            images = [img.to(DEVICE) for img in images]\n",
    "            outputs = model(images)\n",
    "\n",
    "            for i, output in enumerate(outputs):\n",
    "                pred_boxes = output.get(\"boxes\", torch.empty(0)).cpu().numpy()\n",
    "                pred_labels = output.get(\"labels\", torch.empty(0)).cpu().numpy()\n",
    "                true_boxes = targets[i][\"boxes\"].cpu().numpy()\n",
    "                true_labels = targets[i][\"labels\"].cpu().numpy()\n",
    "\n",
    "                # Align predictions and targets\n",
    "                pred_labels = pred_labels[:len(true_labels)]  # Truncate predictions to match targets\n",
    "                all_targets.extend(true_labels.tolist())\n",
    "                all_preds.extend(pred_labels.tolist())\n",
    "\n",
    "                # Append for visualization\n",
    "                if len(predictions) < max_images:\n",
    "                    predictions.append((images[i].cpu(), pred_boxes, pred_labels, true_boxes, true_labels))\n",
    "\n",
    "    return predictions, all_targets, all_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def visualize_predictions_with_opencv(predictions, label_names, save_dir=\"output_images\"):\n",
    "    \"\"\"\n",
    "    Visualize and save predictions using OpenCV for better alignment.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    for idx, (image, pred_boxes, pred_labels, true_boxes, true_labels) in enumerate(predictions):\n",
    "        # Convert the image tensor to a NumPy array\n",
    "        img_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "        img_np = (img_np * 255).astype(np.uint8)  # Scale to 0-255\n",
    "        img_np = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)  # Convert to BGR for OpenCV\n",
    "\n",
    "        img_height, img_width, _ = img_np.shape\n",
    "\n",
    "        # Draw true bounding boxes (Green)\n",
    "        for box, label in zip(true_boxes, true_labels):\n",
    "            x_center, y_center, width, height = box\n",
    "            xmin = int((x_center - width / 2) * img_width)\n",
    "            ymin = int((y_center - height / 2) * img_height)\n",
    "            xmax = int((x_center + width / 2) * img_width)\n",
    "            ymax = int((y_center + height / 2) * img_height)\n",
    "            label_name = label_names[label] if label < len(label_names) else \"Unknown\"\n",
    "            cv2.rectangle(img_np, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)  # Green box\n",
    "            cv2.putText(img_np, f\"True: {label_name}\", (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # Draw predicted bounding boxes (Red)\n",
    "        for box, label in zip(pred_boxes, pred_labels):\n",
    "            x_center, y_center, width, height = box\n",
    "            xmin = int((x_center - width / 2) * img_width)\n",
    "            ymin = int((y_center - height / 2) * img_height)\n",
    "            xmax = int((x_center + width / 2) * img_width)\n",
    "            ymax = int((y_center + height / 2) * img_height)\n",
    "            label_name = label_names[label] if label < len(label_names) else \"Unknown\"\n",
    "            cv2.rectangle(img_np, (xmin, ymin), (xmax, ymax), (0, 0, 255), 2)  # Red box\n",
    "            cv2.putText(img_np, f\"Pred: {label_name}\", (xmin, ymax + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "\n",
    "        # Save the image to a directory\n",
    "        save_path = os.path.join(save_dir, f\"image_{idx + 1}.jpg\")\n",
    "        cv2.imwrite(save_path, img_np)\n",
    "\n",
    "        print(f\"Saved image to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_visualize_metrics(all_targets, all_preds):\n",
    "    \"\"\"\n",
    "    Evaluate and visualize metrics (accuracy, precision, recall, F1 score).\n",
    "    \"\"\"\n",
    "    # Ensure alignment of predictions and targets\n",
    "    min_length = min(len(all_targets), len(all_preds))\n",
    "    all_targets = all_targets[:min_length]\n",
    "    all_preds = all_preds[:min_length]\n",
    "\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    precision = precision_score(all_targets, all_preds, average=\"weighted\", zero_division=0)\n",
    "    recall = recall_score(all_targets, all_preds, average=\"weighted\", zero_division=0)\n",
    "    f1 = f1_score(all_targets, all_preds, average=\"weighted\", zero_division=0)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Plot metrics\n",
    "    metrics = {\"Precision\": precision, \"Recall\": recall, \"F1 Score\": f1}\n",
    "    plt.bar(metrics.keys(), metrics.values(), color=['blue', 'orange', 'green'])\n",
    "    plt.title(\"Evaluation Metrics\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings from E:\\My Research Project\\CODE\\glove.6B.300d.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_25588\\1043328546.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from checkpoint.\n",
      "Saved image to output_images\\image_1.jpg\n",
      "Saved image to output_images\\image_2.jpg\n",
      "Saved image to output_images\\image_3.jpg\n",
      "Saved image to output_images\\image_4.jpg\n",
      "Saved image to output_images\\image_5.jpg\n",
      "Saved image to output_images\\image_6.jpg\n",
      "Saved image to output_images\\image_7.jpg\n",
      "Saved image to output_images\\image_8.jpg\n",
      "Saved image to output_images\\image_9.jpg\n",
      "Saved image to output_images\\image_10.jpg\n",
      "Accuracy: 0.2105\n",
      "Precision: 0.0921\n",
      "Recall: 0.2105\n",
      "F1 Score: 0.1281\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    checkpoint_path = r\"E:\\My Research Project\\CODE\\model_checkpoint.pth\"\n",
    "    test_loader = get_data_loader(DATASET_PATH, \"test\", batch_size=1)\n",
    "\n",
    "    # Load model from checkpoint\n",
    "    model = load_model_from_checkpoint(checkpoint_path, NUM_CLASSES)\n",
    "\n",
    "    # Test the model\n",
    "    predictions, all_targets, all_preds = test_model(model, test_loader, max_images=10)\n",
    "\n",
    "    # Visualize predictions\n",
    "    labelnames =  ['EXCAVATORS', 'dump truck', 'wheel loader', 'Ladder', 'Person', 'Fall-Detected', 'Gloves', 'Goggles', 'Hardhat', 'Ladder', 'Mask', 'NO-Gloves', 'NO-Goggles', 'NO-Hardhat', 'NO-Mask', 'NO-Safety Vest', 'Person', 'Safety Cone', 'Safety Vest'] \n",
    "    visualize_predictions_with_opencv(predictions,labelnames)\n",
    "\n",
    "    # Evaluate and visualize metrics\n",
    "    evaluate_and_visualize_metrics(all_targets, all_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NewGPUTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
