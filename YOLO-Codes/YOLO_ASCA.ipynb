{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset YAML saved at data_config.yaml\n",
      "WARNING  no model scale passed. Assuming scale='n'.\n",
      "Layer 0: Module = <class 'ultralytics.nn.modules.conv.Conv'>, Input Channels = 3, Output Channels = 16, Arguments = [3, 16, 3, 2]\n",
      "Layer 1: Module = <class 'ultralytics.nn.modules.conv.Conv'>, Input Channels = 16, Output Channels = 32, Arguments = [16, 32, 3, 2]\n",
      "Layer 2: Module = <class 'ultralytics.nn.modules.block.C2f'>, Input Channels = 32, Output Channels = 32, Arguments = [32, 32, 1, True]\n",
      "Layer 3: Module = <class 'ultralytics.nn.modules.conv.Conv'>, Input Channels = 32, Output Channels = 64, Arguments = [32, 64, 3, 2]\n",
      "Layer 4: Module = <class 'ultralytics.nn.modules.block.C3k2'>, Input Channels = 64, Output Channels = 64, Arguments = [64, 64, 2, False, 0.25]\n",
      "Layer 5: Module = <class 'ultralytics.nn.modules.conv.Conv'>, Input Channels = 64, Output Channels = 128, Arguments = [64, 128, 3, 2]\n",
      "Layer 6: Module = <class 'ultralytics.nn.modules.block.C3k2'>, Input Channels = 128, Output Channels = 128, Arguments = [128, 128, 2, False, 0.25]\n",
      "Layer 7: Module = <class 'ultralytics.nn.modules.conv.Conv'>, Input Channels = 128, Output Channels = 256, Arguments = [128, 256, 3, 2]\n",
      "Layer 8: Module = <class 'ultralytics.nn.modules.block.C2f'>, Input Channels = 256, Output Channels = 256, Arguments = [256, 256, 1, True]\n",
      "Layer 9: Module = <class 'ultralytics.nn.modules.block.SPPF'>, Input Channels = 256, Output Channels = 256, Arguments = [256, 256, 5]\n",
      "Layer 10: Module = <class 'ultralytics.nn.modules.block.C2PSA'>, Input Channels = 256, Output Channels = 256, Arguments = [256, 256, 1]\n",
      "Layer 11: Module = <class 'ultralytics.nn.modules.conv.Conv'>, Input Channels = 256, Output Channels = 128, Arguments = [256, 128, 1, 1]\n",
      "Layer 12: Module = <class 'torch.nn.modules.upsampling.Upsample'>, Input Channels = 256, Output Channels = 128, Arguments = [None, 2, 'nearest']\n",
      "Layer 13: Module = <class 'ultralytics.nn.modules.conv.Concat'>, Input Channels = 256, Output Channels = 256, Arguments = [1]\n",
      "Layer 14: Module = <class 'ultralytics.nn.modules.block.C2f'>, Input Channels = 256, Output Channels = 128, Arguments = [256, 128, 1]\n",
      "Layer 15: Module = <class 'ultralytics.nn.modules.conv.Conv'>, Input Channels = 128, Output Channels = 64, Arguments = [128, 64, 1, 1]\n",
      "Layer 16: Module = <class 'torch.nn.modules.upsampling.Upsample'>, Input Channels = 128, Output Channels = 64, Arguments = [None, 2, 'nearest']\n",
      "Layer 17: Module = <class 'ultralytics.nn.modules.conv.Concat'>, Input Channels = 128, Output Channels = 128, Arguments = [1]\n",
      "Layer 18: Module = <class 'ultralytics.nn.modules.block.C2f'>, Input Channels = 128, Output Channels = 64, Arguments = [128, 64, 1]\n",
      "Layer 19: Module = <class 'ultralytics.nn.modules.conv.Conv'>, Input Channels = 64, Output Channels = 32, Arguments = [64, 32, 1, 1]\n",
      "Layer 20: Module = <class 'torch.nn.modules.upsampling.Upsample'>, Input Channels = 64, Output Channels = 32, Arguments = [None, 2, 'nearest']\n",
      "Layer 21: Module = <class 'ultralytics.nn.modules.conv.Concat'>, Input Channels = 64, Output Channels = 64, Arguments = [1]\n",
      "Layer 22: Module = <class 'ultralytics.nn.modules.block.C2f'>, Input Channels = 64, Output Channels = 32, Arguments = [64, 32, 1]\n",
      "Layer 23: Module = <class 'ultralytics.nn.modules.head.Detect'>, Input Channels = 64, Output Channels = 32, Arguments = [99, [64, 32, 32]]\n",
      "Checkpoint not found! Starting fresh training.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ultralytics import YOLO\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths\n",
    "DATASET_PATH = r\"E:\\My Research Project\\CODE\\DataCombinedWithConstruction\"  \n",
    "CUSTOM_MODEL_YAML = \"yolo_asca.yaml\"  \n",
    "CHECKPOINT_PATH = \"yolo_asca_results/yolo_asca_run/weights/last.pt\"  \n",
    "FINAL_MODEL_PATH = \"yolo_asca_results/yolo_asca_run/weights/final.pt\"  \n",
    "BATCH_SIZE = 2  # Batch size\n",
    "\n",
    "# --------------------------- Generate Dataset YAML ---------------------------\n",
    "data_config = {\n",
    "    \"train\": os.path.join(DATASET_PATH, \"train\"),  # Path to training images\n",
    "    \"val\": os.path.join(DATASET_PATH, \"val\"),      # Path to validation images\n",
    "    \"nc\": 99,                                      # Number of classesx \n",
    "    \"names\": [f\"{i}\" for i in range(99)]     # Dummy class names\n",
    "}\n",
    "\n",
    "# Save the dataset configuration as a YAML file\n",
    "dataset_yaml_path = \"data_config.yaml\"\n",
    "with open(dataset_yaml_path, \"w\") as file:\n",
    "    yaml.dump(data_config, file, default_flow_style=False)\n",
    "print(f\"Dataset YAML saved at {dataset_yaml_path}\")\n",
    "\n",
    "# --------------------------- Load YOLO Model ---------------------------\n",
    "# Load the YOLO model with the custom YAML file\n",
    "model = YOLO(CUSTOM_MODEL_YAML, task=\"detect\")\n",
    "\n",
    "# --------------------------- Training with Resume ---------------------------\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    print(f\"Resuming training from checkpoint: {CHECKPOINT_PATH}\")\n",
    "    model.load(CHECKPOINT_PATH)  # Load the last checkpoint\n",
    "else:\n",
    "    print(\"Checkpoint not found! Starting fresh training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.58 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.50  Python-3.11.10 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolo_asca.yaml, data=data_config.yaml, epochs=20, time=None, patience=100, batch=2, imgsz=640, save=True, save_period=1, cache=False, device=0, workers=8, project=yolo_asca_results, name=yolo_asca_run6, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=yolo_asca_results\\yolo_asca_run6\n",
      "WARNING  no model scale passed. Assuming scale='n'.\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "Layer 0: Module = <class 'ultralytics.nn.modules.conv.Conv'>, Input Channels = 3, Output Channels = 16, Arguments = [3, 16, 3, 2]\n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "Layer 1: Module = <class 'ultralytics.nn.modules.conv.Conv'>, Input Channels = 16, Output Channels = 32, Arguments = [16, 32, 3, 2]\n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "Layer 2: Module = <class 'ultralytics.nn.modules.block.C2f'>, Input Channels = 32, Output Channels = 32, Arguments = [32, 32, 1, True]\n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "Layer 3: Module = <class 'ultralytics.nn.modules.conv.Conv'>, Input Channels = 32, Output Channels = 64, Arguments = [32, 64, 3, 2]\n",
      "  4                  -1  2     11040  ultralytics.nn.modules.block.C3k2            [64, 64, 2, False, 0.25]      \n",
      "Layer 4: Module = <class 'ultralytics.nn.modules.block.C3k2'>, Input Channels = 64, Output Channels = 64, Arguments = [64, 64, 2, False, 0.25]\n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "Layer 5: Module = <class 'ultralytics.nn.modules.conv.Conv'>, Input Channels = 64, Output Channels = 128, Arguments = [64, 128, 3, 2]\n",
      "  6                  -1  2     43584  ultralytics.nn.modules.block.C3k2            [128, 128, 2, False, 0.25]    \n",
      "Layer 6: Module = <class 'ultralytics.nn.modules.block.C3k2'>, Input Channels = 128, Output Channels = 128, Arguments = [128, 128, 2, False, 0.25]\n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "Layer 7: Module = <class 'ultralytics.nn.modules.conv.Conv'>, Input Channels = 128, Output Channels = 256, Arguments = [128, 256, 3, 2]\n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "Layer 8: Module = <class 'ultralytics.nn.modules.block.C2f'>, Input Channels = 256, Output Channels = 256, Arguments = [256, 256, 1, True]\n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      "Layer 9: Module = <class 'ultralytics.nn.modules.block.SPPF'>, Input Channels = 256, Output Channels = 256, Arguments = [256, 256, 5]\n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      "Layer 10: Module = <class 'ultralytics.nn.modules.block.C2PSA'>, Input Channels = 256, Output Channels = 256, Arguments = [256, 256, 1]\n",
      " 11                  -1  1     33024  ultralytics.nn.modules.conv.Conv             [256, 128, 1, 1]              \n",
      "Layer 11: Module = <class 'ultralytics.nn.modules.conv.Conv'>, Input Channels = 256, Output Channels = 128, Arguments = [256, 128, 1, 1]\n",
      " 12                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      "Layer 12: Module = <class 'torch.nn.modules.upsampling.Upsample'>, Input Channels = 256, Output Channels = 128, Arguments = [None, 2, 'nearest']\n",
      " 13             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      "Layer 13: Module = <class 'ultralytics.nn.modules.conv.Concat'>, Input Channels = 256, Output Channels = 256, Arguments = [1]\n",
      " 14                  -1  1    131840  ultralytics.nn.modules.block.C2f             [256, 128, 1]                 \n",
      "Layer 14: Module = <class 'ultralytics.nn.modules.block.C2f'>, Input Channels = 256, Output Channels = 128, Arguments = [256, 128, 1]\n",
      " 15                  -1  1      8320  ultralytics.nn.modules.conv.Conv             [128, 64, 1, 1]               \n",
      "Layer 15: Module = <class 'ultralytics.nn.modules.conv.Conv'>, Input Channels = 128, Output Channels = 64, Arguments = [128, 64, 1, 1]\n",
      " 16                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      "Layer 16: Module = <class 'torch.nn.modules.upsampling.Upsample'>, Input Channels = 128, Output Channels = 64, Arguments = [None, 2, 'nearest']\n",
      " 17             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      "Layer 17: Module = <class 'ultralytics.nn.modules.conv.Concat'>, Input Channels = 128, Output Channels = 128, Arguments = [1]\n",
      " 18                  -1  1     33152  ultralytics.nn.modules.block.C2f             [128, 64, 1]                  \n",
      "Layer 18: Module = <class 'ultralytics.nn.modules.block.C2f'>, Input Channels = 128, Output Channels = 64, Arguments = [128, 64, 1]\n",
      " 19                  -1  1      2112  ultralytics.nn.modules.conv.Conv             [64, 32, 1, 1]                \n",
      "Layer 19: Module = <class 'ultralytics.nn.modules.conv.Conv'>, Input Channels = 64, Output Channels = 32, Arguments = [64, 32, 1, 1]\n",
      " 20                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      "Layer 20: Module = <class 'torch.nn.modules.upsampling.Upsample'>, Input Channels = 64, Output Channels = 32, Arguments = [None, 2, 'nearest']\n",
      " 21             [-1, 2]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      "Layer 21: Module = <class 'ultralytics.nn.modules.conv.Concat'>, Input Channels = 64, Output Channels = 64, Arguments = [1]\n",
      " 22                  -1  1      8384  ultralytics.nn.modules.block.C2f             [64, 32, 1]                   \n",
      "Layer 22: Module = <class 'ultralytics.nn.modules.block.C2f'>, Input Channels = 64, Output Channels = 32, Arguments = [64, 32, 1]\n",
      " 23        [16, 19, 22]  1    275222  ultralytics.nn.modules.head.Detect           [99, [64, 32, 32]]            \n",
      "Layer 23: Module = <class 'ultralytics.nn.modules.head.Detect'>, Input Channels = 64, Output Channels = 32, Arguments = [99, [64, 32, 32]]\n",
      "YOLO_asca summary: 267 layers, 1,821,766 parameters, 1,821,750 gradients, 10.6 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir yolo_asca_results\\yolo_asca_run6', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning E:\\My Research Project\\CODE\\DataCombinedWithConstruction\\train\\labels... 40698 images, 811 backgrounds, 0 corrupt: 100%|██████████| 40698/40698 [02:08<00:00, 317.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  E:\\My Research Project\\CODE\\DataCombinedWithConstruction\\train\\images\\2128.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  E:\\My Research Project\\CODE\\DataCombinedWithConstruction\\train\\images\\2841.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  E:\\My Research Project\\CODE\\DataCombinedWithConstruction\\train\\images\\4304.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  E:\\My Research Project\\CODE\\DataCombinedWithConstruction\\train\\images\\4305.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  E:\\My Research Project\\CODE\\DataCombinedWithConstruction\\train\\images\\4356.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  E:\\My Research Project\\CODE\\DataCombinedWithConstruction\\train\\images\\6661.jpg: 2 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: E:\\My Research Project\\CODE\\DataCombinedWithConstruction\\train\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\My Research Project\\CODE\\DataCombinedWithConstruction\\val\\labels... 9212 images, 202 backgrounds, 0 corrupt: 100%|██████████| 9212/9212 [00:24<00:00, 374.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\My Research Project\\CODE\\DataCombinedWithConstruction\\val\\labels.cache\n",
      "Plotting labels to yolo_asca_results\\yolo_asca_run6\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 67 weight(decay=0.0), 74 weight(decay=0.0005), 73 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1myolo_asca_results\\yolo_asca_run6\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/20       1.2G      3.506      6.468      3.862          8        640:  62%|██████▏   | 12577/20349 [30:07<18:36,  6.96it/s] \n"
     ]
    },
    {
     "ename": "error",
     "evalue": "Caught error in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\ASUS\\anaconda3\\envs\\NewGPUTorch\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\ASUS\\anaconda3\\envs\\NewGPUTorch\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\ASUS\\anaconda3\\envs\\NewGPUTorch\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"c:\\Users\\ASUS\\anaconda3\\envs\\NewGPUTorch\\Lib\\site-packages\\ultralytics\\data\\base.py\", line 288, in __getitem__\n    return self.transforms(self.get_image_and_label(index))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\ASUS\\anaconda3\\envs\\NewGPUTorch\\Lib\\site-packages\\ultralytics\\data\\base.py\", line 294, in get_image_and_label\n    label[\"img\"], label[\"ori_shape\"], label[\"resized_shape\"] = self.load_image(index)\n                                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\ASUS\\anaconda3\\envs\\NewGPUTorch\\Lib\\site-packages\\ultralytics\\data\\base.py\", line 163, in load_image\n    im = cv2.imread(f)  # BGR\n         ^^^^^^^^^^^^^\n  File \"c:\\Users\\ASUS\\anaconda3\\envs\\NewGPUTorch\\Lib\\site-packages\\ultralytics\\utils\\patches.py\", line 26, in imread\n    return cv2.imdecode(np.fromfile(filename, np.uint8), flags)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ncv2.error: OpenCV(4.10.0) C:\\b\\abs_daut97tdpo\\croot\\opencv-suite_1722029138522\\work\\modules\\core\\src\\alloc.cpp:73: error: (-4:Insufficient memory) Failed to allocate 1228800 bytes in function 'cv::OutOfMemoryError'\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model and save checkpoints\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_yaml_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Path to dataset YAML file\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# Number of epochs\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Input image size (640x640)\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Batch size\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# GPU device index\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myolo_asca_run\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Experiment name\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myolo_asca_results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Directory to save results\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCHECKPOINT_PATH\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Resume if checkpoint exists\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# Save checkpoint after every epoch\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# Save best model checkpoint\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# --------------------------- Evaluation ---------------------------\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Validate the model after training\u001b[39;00m\n\u001b[0;32m     17\u001b[0m metrics \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mval()\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\NewGPUTorch\\Lib\\site-packages\\ultralytics\\engine\\model.py:806\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[1;32m--> 806\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\NewGPUTorch\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:207\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 207\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\NewGPUTorch\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:362\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    360\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m TQDM(\u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader), total\u001b[38;5;241m=\u001b[39mnb)\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 362\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_train_batch_start\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Warmup\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\NewGPUTorch\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\NewGPUTorch\\Lib\\site-packages\\ultralytics\\data\\build.py:48\u001b[0m, in \u001b[0;36mInfiniteDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a sampler that repeats indefinitely.\"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)):\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\NewGPUTorch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\NewGPUTorch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1465\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1464\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[1;32m-> 1465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\NewGPUTorch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1491\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[0;32m   1490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[1;32m-> 1491\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\NewGPUTorch\\Lib\\site-packages\\torch\\_utils.py:715\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[0;32m    714\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 715\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[1;31merror\u001b[0m: Caught error in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\ASUS\\anaconda3\\envs\\NewGPUTorch\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\ASUS\\anaconda3\\envs\\NewGPUTorch\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\ASUS\\anaconda3\\envs\\NewGPUTorch\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"c:\\Users\\ASUS\\anaconda3\\envs\\NewGPUTorch\\Lib\\site-packages\\ultralytics\\data\\base.py\", line 288, in __getitem__\n    return self.transforms(self.get_image_and_label(index))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\ASUS\\anaconda3\\envs\\NewGPUTorch\\Lib\\site-packages\\ultralytics\\data\\base.py\", line 294, in get_image_and_label\n    label[\"img\"], label[\"ori_shape\"], label[\"resized_shape\"] = self.load_image(index)\n                                                               ^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\ASUS\\anaconda3\\envs\\NewGPUTorch\\Lib\\site-packages\\ultralytics\\data\\base.py\", line 163, in load_image\n    im = cv2.imread(f)  # BGR\n         ^^^^^^^^^^^^^\n  File \"c:\\Users\\ASUS\\anaconda3\\envs\\NewGPUTorch\\Lib\\site-packages\\ultralytics\\utils\\patches.py\", line 26, in imread\n    return cv2.imdecode(np.fromfile(filename, np.uint8), flags)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ncv2.error: OpenCV(4.10.0) C:\\b\\abs_daut97tdpo\\croot\\opencv-suite_1722029138522\\work\\modules\\core\\src\\alloc.cpp:73: error: (-4:Insufficient memory) Failed to allocate 1228800 bytes in function 'cv::OutOfMemoryError'\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model and save checkpoints\n",
    "results = model.train(\n",
    "    data=dataset_yaml_path,  # Path to dataset YAML file\n",
    "    epochs=20   ,                # Number of epochs\n",
    "    imgsz=640,               # Input image size (640x640)\n",
    "    batch=BATCH_SIZE,        # Batch size\n",
    "    device=0,                # GPU device index\n",
    "    name=\"yolo_asca_run\",    # Experiment name\n",
    "    project=\"yolo_asca_results\",  # Directory to save results\n",
    "    resume=os.path.exists(CHECKPOINT_PATH),  # Resume if checkpoint exists\n",
    "    save_period=1,           # Save checkpoint after every epoch\n",
    "    save=True                # Save best model checkpoint\n",
    ")\n",
    "\n",
    "# --------------------------- Evaluation ---------------------------\n",
    "# Validate the model after training\n",
    "metrics = model.val()\n",
    "print(f\"Validation Results: {metrics}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NewGPUTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
